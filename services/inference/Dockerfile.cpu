FROM python:3.11-slim

WORKDIR /app

# Install llama-cpp-python for CPU inference
RUN pip install --no-cache-dir \
    llama-cpp-python \
    fastapi \
    uvicorn \
    prometheus-client

COPY src/ /app/

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
